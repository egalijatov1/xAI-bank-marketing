{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport eli5\nfrom scipy import stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv('/kaggle/input/bank-marketing/bank-additional-full.csv', sep = ';')\n\ntrain.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train[\"y\"].map({\"no\":0, \"yes\":1})\nX = train.drop(\"y\", axis=1)\nX.columns\nX.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data exploration**","metadata":{}},{"cell_type":"code","source":"count = len(X)\nprint(count)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(3,4)\nsns.countplot(x = 'y', data = train)\nax.set_xlabel('y', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nsns.despine()\n\nno_ = len(train[train['y'] == 'no'])\nyes_ = len(train[train['y'] == 'yes'])\nprint(\"NO:\", no_, \"which is\", (no_/count)*100, \"%\")\nprint(\"YES:\", yes_, \"which is\", (yes_/count)*100, \"%\")\n      ","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset is highly imbalanced! The response rate is only 11.6%. Hence the Y variable has a high class imbalance. Because of this accuracy will not be a reliable model performance measure. \n\n1. One solution is to do over-sampling using SMOTE and create a balanced dataset\n2. Other solution is to use precision or recall as the performance measures, and decide which one is more important (to have less FN or less FP)\n\nIn this particular problem, where model is trying to predict wether a person will subscribe or not, more damage would be caused by false positives. This is because bank might be counting on more subscribed clients, when acctually there are less then predicted. This is the reason that precision will be used as evaluation method.","metadata":{}},{"cell_type":"markdown","source":"From dataset description:\n\n* duration: \n\n\"last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\"\n\nBecause of this we will drop this column!","metadata":{}},{"cell_type":"code","source":"X.drop(\"duration\", inplace=True, axis=1)\ntrain.drop(\"duration\", inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1. AGE**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(23, 8)\nsns.countplot(x = 'age', data = train)\nax.set_xlabel('Age', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Age Count Distribution', fontsize=15)\nsns.despine()\n\ntrain['age'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Quartiles\n# print('1º Quartile: ', train['age'].quantile(q = 0.25))\n# print('2º Quartile: ', train['age'].quantile(q = 0.50))\n# print('3º Quartile: ', train['age'].quantile(q = 0.75))\n# print('4º Quartile: ', train['age'].quantile(q = 1.00))\n# #Calculate the outliers:\n#   # Interquartile range, IQR = Q3 - Q1\n#   # lower 1.5*IQR whisker = Q1 - 1.5 * IQR \n#   # Upper 1.5*IQR whisker = Q3 + 1.5 * IQR\n    \n# print('Ages above: ', train['age'].quantile(q = 0.75) + \n#                       1.5*(train['age'].quantile(q = 0.75) - train['age'].quantile(q = 0.25)), 'are outliers')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print('Numerber of outliers: ', len(train[train['age'] > 69.6]))\n# print('Outliers are:', round(len(train[train['age'] > 69.6])*100/count,2), '%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sns.distplot(train['age']);\n# fig = plt.figure()\n# res = stats.probplot(train['age'], plot=plt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"YES age mean:\",train[train['y'] == 'yes']['age'].mean())\nprint(\"NO age mean:\", train[train['y'] == 'no']['age'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['age'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The average age of customers who bought the term deposit is slightly higher than that of the customers who didn’t.\n\nWe will transform this feature using RobustScaler to reduce the influence of the outliers instead of dropping them since we might loose some usefull information!","metadata":{}},{"cell_type":"markdown","source":"**2. JOB**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(23, 8)\nsns.countplot(x = 'job', data = train)\nax.set_xlabel('Job', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Job Count Distribution', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('job').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a categorical variable and since it does not form any logical sequence, we will use OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['job'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The highest conversion is for students (31%) and lowest is for blue-collar(7%)!","metadata":{}},{"cell_type":"code","source":"pd.crosstab(train.job,train.y).plot(kind='bar')\nplt.title('Purchase Frequency for Job Title')\nplt.xlabel('Job')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('purchase_fre_job')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The frequency of purchase of the deposit vastly depends on the job title. Thus, the job title can be a good predictor of the outcome variable.","metadata":{}},{"cell_type":"markdown","source":"**3. MARITAL**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(5, 5)\nsns.countplot(x = 'marital', data = train)\nax.set_xlabel('Marital', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Marital', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nprint('Married:', len(train[train['marital'] == 'married']))\nprint('Single:' , len(train[train['marital'] == 'single']))\nprint('Divorced:' , len(train[train['marital'] == 'divorced']))\nprint('Unknown:', len(train[train['marital'] == 'unknown']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.groupby('marital').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same as before, for this variable we will use OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['marital'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table=pd.crosstab(train.marital,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Marital Status vs Purchase')\nplt.xlabel('Marital Status')\nplt.ylabel('Proportion of Customers')\nplt.savefig('mariral_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The marital status does not seem a strong predictor for the outcome variable.","metadata":{}},{"cell_type":"markdown","source":"**4. EDUCATION**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(17, 5)\nsns.countplot(x = 'education', data = train)\nax.set_xlabel('Education', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Education Count Distribution', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['education'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table=pd.crosstab(train.education,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Education vs Purchase')\nplt.xlabel('Education')\nplt.ylabel('Proportion of Customers')\nplt.savefig('edu_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"basic.4y basic.6y and bascic.9y have similar meaning and behaviour when compared to target variable so we can bind them into one value - basic.","metadata":{}},{"cell_type":"code","source":"X['education']=np.where(X['education'] =='basic.9y', 'basic', X['education'])\nX['education']=np.where(X['education'] =='basic.6y', 'basic', X['education'])\nX['education']=np.where(X['education'] =='basic.4y', 'basic', X['education'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.groupby('education').mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(X['education'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, we will use OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"table=pd.crosstab(X.education,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Education vs Purchase')\nplt.xlabel('Education')\nplt.ylabel('Proportion of Customers')\nplt.savefig('edu_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Education seems a good predictor of the outcome variable.","metadata":{}},{"cell_type":"markdown","source":"**5. DEFAULT**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(3, 4)\nsns.countplot(x = 'default', data = train, order = ['no', 'yes', 'unknown'])\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Default', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('No credit in default:', len(train[train['default'] == 'no']))\nprint('Yes to credit in default:' , len(train[train['default'] == 'yes']), \"which is:\", 100*(len(train[train['default'] == 'yes'])/count),\"%\")\nprint('Unknown credit in default:', len(train[train['default'] == 'unknown']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['default'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since this feature - yes is only 0.073% of the data and the conversion is also comparitively lower for default - yes, we can remove this column!","metadata":{}},{"cell_type":"code","source":"X.drop(\"default\", inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The solution was tested with and without this column, and it show that this approach was right and that results are better without this column!","metadata":{}},{"cell_type":"markdown","source":"**6. HOUSING**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(3,4)\nsns.countplot(x = 'housing', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Housing', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nprint('No housing in loan:', len(train[train['housing'] == 'no']))\nprint('Yes housing in loan:' , len(train[train['housing'] == 'yes']))\nprint('Unknown housing in loan:', len(train[train['housing'] == 'unknown']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also encode this feature with OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['housing'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table=pd.crosstab(train.housing,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Housing vs Purchase')\nplt.xlabel('Housing')\nplt.ylabel('Proportion of Customers')\nplt.savefig('housing_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Housing does not seem like a good predictor.","metadata":{}},{"cell_type":"markdown","source":"**7. LOAN**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(3,4)\nsns.countplot(x = 'loan', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Loan', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nprint('No to personal loan:', len(train[train['loan'] == 'no']))\nprint('Yes to personal loan:' , len(train[train['loan'] == 'yes']))\nprint('Unknown to personal loan:', len(train[train['loan'] == 'unknown']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encoding will be done with OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['loan'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table=pd.crosstab(train.loan,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Loan vs Purchase')\nplt.xlabel('Loan')\nplt.ylabel('Proportion of Customers')\nplt.savefig('loan_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loan does not seem like a good predictor!","metadata":{}},{"cell_type":"markdown","source":"**8. CONTACT**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(3,4)\nsns.countplot(x = 'contact', data =train )\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Contact', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nprint('Telephone:', len(train[train['contact'] == 'telephone']))\nprint('Celular:' , len(train[train['contact'] == 'cellular']))\nprint('Missing:', count - len(train[train['contact'] == 'telephone']) - len(X[X['contact'] == 'cellular']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['contact'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table=pd.crosstab(train.contact,train.y)\ntable.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\nplt.title('Stacked Bar Chart of Contact vs Purchase')\nplt.xlabel('Contact')\nplt.ylabel('Proportion of Customers')\nplt.savefig('contact_vs_pur_stack')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Contact variable could be a good predictor.","metadata":{}},{"cell_type":"markdown","source":"**9. Month and day of last contact**","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20,8))\nsns.countplot(x = 'month', data = train, ax = ax1)\nax1.set_title('Month', fontsize=15)\nax1.set_xlabel('')\nax1.set_ylabel('Count', fontsize=15)\nax1.tick_params(labelsize=15)\n\n# Housing, has housing loan ?\nsns.countplot(x = 'day_of_week', data = train, ax = ax2)\nax2.set_title('Day of Week', fontsize=15)\nax2.set_xlabel('')\nax2.set_ylabel('Count', fontsize=15)\nax2.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In contrast to previous categorical features, these two do form some sequence so we will try our solution with LabelEncoder and with OneHotEncoder!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['day_of_week'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(train.day_of_week,train.y).plot(kind='bar')\nplt.title('Purchase Frequency for Day of Week')\nplt.xlabel('Day of Week')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('pur_dayofweek_bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Day of week may not be a good predictor of the outcome.","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['month'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(train.month,train.y).plot(kind='bar')\nplt.title('Purchase Frequency for Month')\nplt.xlabel('Month')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('pur_fre_month_bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Month might be a good predictor of the outcome variable.","metadata":{}},{"cell_type":"markdown","source":"**10. Previous Outcome**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(5,4)\nsns.countplot(x = 'poutcome', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Poutcome', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nprint('Nonexistent outcome:', len(train[train['poutcome'] == 'nonexistent']), \"which is\", (len(X[X['poutcome'] == 'nonexistent'])/count)*100, \"%\")\nprint('Failed outcome:', len(train[train['poutcome'] == 'failure']))\nprint('Success outcome:', len(train[train['poutcome'] == 'success']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a lot of nonexistent values in this column (86.3%) but we hope that the values that are known carry important information about the client. We will encode this with OneHotEncoder.","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['poutcome'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(train.poutcome,train.y).plot(kind='bar')\nplt.title('Purchase Frequency for Poutcome')\nplt.xlabel('Poutcome')\nplt.ylabel('Frequency of Purchase')\nplt.savefig('pur_fre_pout_bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Poutcome seems to be a good predictor of the outcome variable.","metadata":{}},{"cell_type":"markdown","source":"**11. PDAYS**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(20,4)\nsns.countplot(x = 'pdays', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('pdays', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_unknown_pdays = len(train[train['pdays'] == 999])\ncount = len(X)\nprint('Percent of unknown pdays: ', (count_unknown_pdays/count)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since almost all entries have unknown pdays attribute we could conclude to drop this column since it seems like it does not carry a lot of information. ","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['pdays'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pdays (days since the customer was last contacted) is understandably lower for the customers who bought it. The lower the pdays, the better the memory of the last call and hence the better chances of a sale.","metadata":{}},{"cell_type":"markdown","source":"From bivariate analysis we see that pdays seems important for the resulting column, but this may be the product of many missing values. The results will be compared with and without this column.","metadata":{}},{"cell_type":"code","source":"#Certain variables are more relevant if they are categorical variable than numerical variables. \n#Because of that we will try our results when we convert this variable to categoric!\n#X['pdays']=X['pdays'].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**12. PREVIOUS**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(5,4)\nsns.countplot(x = 'previous', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('previous', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From dataset description:\n> Previous: number of contacts performed before this campaign and for this client (numeric)","metadata":{}},{"cell_type":"code","source":"no_previous = len(train[train['previous'] == 0])\nprint('Percent of no previous: ', (no_previous/count)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that not many clients had been contacted before this campaign!","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['previous'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**13. CAMPAIGN**","metadata":{}},{"cell_type":"markdown","source":"> Campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)","metadata":{}},{"cell_type":"code","source":"sns.distplot(train['campaign']);\nfig = plt.figure()\n#res = stats.probplot(train['campaign'], plot=plt)\n\ntrain['campaign'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x=train['campaign'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(figsize=(10,5))\n# ax.scatter(train['campaign'], y)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Percent of campaign = 1: ', (len(train[train['campaign'] == 1])/count)*100, \"%\")\nprint('Percent of campaign = 2: ', (len(train[train['campaign'] == 2])/count)*100, \"%\")\nprint('Percent of campaign = 3: ', (len(train[train['campaign'] == 3])/count)*100, \"%\")\nprint('Percent of campaign >= 4: ', (len(train[train['campaign'] >= 4])/count)*100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that most of the values for the campaign are 1,2 and 3. Which we also see from the boxplot. That is why it is a good idea to bind these values into 4 categorical values.","metadata":{}},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['campaign'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This feature has many outliers!\n\nFrom boxplot we also see that aproximately all values X['campaign'] > 10 are potential outliers, and there is 2% of such data, but dropping all that could cause a lost of infromation, so we won't drop them for now!\n\nTo reduce the importance of the outliers we can transform this column to be a categorical variable and to have only 4 possible values:\n* campaign <= 2\n* campaign = 3\n* campaign = 4\n* campaign > 4\n","metadata":{}},{"cell_type":"code","source":"#Binning campaign\ncol = X['campaign']\ncut_points = [2,3,4]\nlabels = [\"<=2\",\"3\",\"4\",\">4\"]\nminval = col.min()\nmaxval = col.max()\n\n#create list by adding min and max to cut_points\nbreak_points = [minval] + cut_points + [maxval]\n\n#Binning using cut function of pandas\ncolBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\nX['campaign_new'] = colBin\nX.drop(['campaign'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, ax = plt.subplots()\n#fig.set_size_inches(5,4)\n#sns.countplot(x = 'campaign_new', data = X)\n#ax.set_xlabel('')\n#ax.set_ylabel('Count', fontsize=15)\n#ax.set_title('campaign_new', fontsize=15)\n#ax.tick_params(labelsize=15)\n#sns.despine()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X.drop(X[X['campaign'] == 56].index, inplace=True)\n#y = pd.DataFrame(y)\n#y.drop(y.index[4107], inplace=True)\n#y = y['y']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**14. Social and economic context attributes**","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(8,4)\nsns.countplot(x = 'emp.var.rate', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('employment variation rate', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nX['emp.var.rate'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['emp.var.rate'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(30,4)\nsns.countplot(x = 'cons.price.idx', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('consumer price index', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nX['cons.price.idx'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['cons.price.idx'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(30,4)\nsns.countplot(x = 'cons.conf.idx', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('consumer confidence index', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nX['cons.conf.idx'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['cons.conf.idx'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(35,4)\nsns.countplot(x = 'euribor3m', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('euribor 3 month rate', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nX['euribor3m'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nfig.set_size_inches(12,4)\nsns.countplot(x = 'nr.employed', data = train)\nax.set_xlabel('')\nax.set_ylabel('Count', fontsize=15)\nax.set_title('number of employees', fontsize=15)\nax.tick_params(labelsize=15)\nsns.despine()\n\nX['nr.employed'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bivariate analysis using crosstab:\npd.crosstab(train['nr.employed'], train['y'], normalize='index').sort_values(by='yes',ascending=False )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#with and without columns:\n#X.drop(\"pdays\", inplace=True, axis=1)\n#X.drop(\"emp.var.rate\", inplace=True, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREPROCESSING","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#numeric features\nnum_features = [\"age\", \"previous\", \"emp.var.rate\",'pdays','campaign_new',\n                 \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]\n\n#categorical features to be encoded with OneHotEncoder:\ncat_oh_features = [\"job\", \"marital\", \"education\", \"housing\", \"loan\", \"contact\", \"poutcome\", \"month\", \"day_of_week\"]\n\n#categorical features to be encoded with LabelEncoder:\ncat_le_features = []#, \"month\", \"day_of_week\"]#, \"pdays\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# min_max = [\"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]\n# m_scaler = MinMaxScaler()\n# X[min_max] = m_scaler.fit_transform(X[min_max])\n# robust = [\"age\", 'pdays','campaign']\n# r_scaler = RobustScaler()\n# X[robust] = r_scaler.fit_transform(X[robust])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\n#X['month'] = le.fit_transform(X['month'])\n#X['day_of_week'] = le.fit_transform(X['day_of_week'])\nX['campaign_new'] = le.fit_transform(X['campaign_new'])\n#X['pdays'] = le.fit_transform(X['pdays'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = StandardScaler()\nX[num_features] = sc.fit_transform(X[num_features])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features+cat_le_features), \n                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n                                   cat_oh_features)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **PREDICTION AND EXPLAINABLE AI**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nimport shap\n\nshap.initjs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Logistic regression\n","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_model = Pipeline([(\"preprocessor\", preprocessor),\n                    (\"model\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_model.fit(X_train, y_train)\ny_pred1 = lr_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame({'Method':['Logistic Regression'], \n                        'accuracy': accuracy_score(y_test, y_pred1), \n                        'precision': precision_score(y_test, y_pred1, pos_label=1),\n                        'recall': recall_score(y_test, y_pred1, pos_label=1),\n                        'f1': f1_score(y_test, y_pred1, pos_label=1)\n                      })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y_test, y_pred1))\nprint(classification_report(y_test, y_pred1))\nprint(confusion_matrix(y_test, y_pred1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to show real feature names, to get real insight in this explanation!","metadata":{}},{"cell_type":"code","source":"preprocessor = lr_model.named_steps[\"preprocessor\"]\nohe_categories = preprocessor.named_transformers_[\"categorical\"].categories_\nnew_ohe_features = [f\"{col}__{val}\" for col, vals in zip(cat_oh_features, ohe_categories) for val in vals]\nall_features = num_features+ cat_le_features + new_ohe_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(lr_model.named_steps[\"preprocessor\"].transform(X_train), columns=all_features).head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ELI5","metadata":{}},{"cell_type":"code","source":"eli5.show_weights(lr_model.named_steps[\"model\"], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_instances = lr_model.named_steps['preprocessor'].fit_transform(X_test)\neli5.explain_prediction(lr_model.named_steps[\"model\"], prep_instances[0] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.explain_prediction(lr_model.named_steps[\"model\"], prep_instances[42] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHAP","metadata":{}},{"cell_type":"code","source":"prep1 = lr_model.named_steps['preprocessor'].fit_transform(X_train)\nexplainer1 = shap.LinearExplainer(lr_model.named_steps[\"model\"], prep1)\nobservations1 = lr_model.named_steps[\"preprocessor\"].transform(X_test.sample(1000, random_state=42))\nshap_values1 = explainer1.shap_values(observations1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nshap.force_plot(explainer1.expected_value, shap_values1[i],\n                features=observations1[i], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer1.expected_value, shap_values1,\n                features=observations1, feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values1, features=observations1, feature_names=all_features, max_display=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Decision trees","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt_model = Pipeline([(\"preprocessor\", preprocessor), \n                     (\"model\", DecisionTreeClassifier(class_weight=\"balanced\"))])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using GridSearch to find best params for decision tree model!","metadata":{}},{"cell_type":"code","source":"gs2 = GridSearchCV(dt_model, {\"model__max_depth\": [3, 5, 7], \n                             \"model__min_samples_split\": [2, 5]}, \n                  cv=5,\n                  n_jobs = -1,\n                  scoring=\"accuracy\")\n\ngs2.fit(X_train, y_train)\nprint(gs2.best_params_)\nprint(gs2.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt_model.set_params(**gs2.best_params_)\ndt_model.fit(X_train, y_train)\ny_pred2 = dt_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame({'Method':['Decision Trees'], \n                        'accuracy': accuracy_score(y_test, y_pred2), \n                        'precision': precision_score(y_test, y_pred2, pos_label=1),\n                        'recall': recall_score(y_test, y_pred2, pos_label=1),\n                        'f1': f1_score(y_test, y_pred2, pos_label=1)\n                      })\nresults = pd.concat([results, temp])\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred2))\nprint(classification_report(y_test, y_pred2))\nprint(confusion_matrix(y_test, y_pred2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ELI5","metadata":{}},{"cell_type":"code","source":"eli5.explain_weights(dt_model.named_steps[\"model\"],feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_instances = dt_model.named_steps['preprocessor'].fit_transform(X_test)\neli5.explain_prediction(dt_model.named_steps[\"model\"], prep_instances[0] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.explain_prediction(dt_model.named_steps[\"model\"], prep_instances[42] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHAP","metadata":{}},{"cell_type":"code","source":"explainer2 = shap.TreeExplainer(dt_model.named_steps[\"model\"])\nobservations2 = dt_model.named_steps[\"preprocessor\"].transform(X_train.sample(1000, random_state=42))\nshap_values2 = explainer2.shap_values(observations2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nshap.force_plot(explainer2.expected_value[i], shap_values2[i], \n                features=observations2, feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values2, features=observations2, feature_names=all_features, max_display=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_model = Pipeline([(\"preprocessor\", preprocessor), \n                     (\"model\", RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, n_jobs=-1))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs3 = GridSearchCV(rf_model, {\"model__max_depth\": [10, 15], \n                             \"model__min_samples_split\": [5, 10]},  \n                  cv=5,\n                  n_jobs = -1,\n                  scoring=\"precision\")\n\ngs3.fit(X_train, y_train)\nprint(gs3.best_params_)\nprint(gs3.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_model.set_params(**gs3.best_params_)\nrf_model.fit(X_train, y_train)\ny_pred3 = rf_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame({'Method':['Random Forest'], \n                        'accuracy': accuracy_score(y_test, y_pred3), \n                        'precision': precision_score(y_test, y_pred3, pos_label=1),\n                        'recall': recall_score(y_test, y_pred3, pos_label=1),\n                          'f1': f1_score(y_test, y_pred3, pos_label=1)\n                      })\nresults = pd.concat([results, temp])\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred3))\nprint(classification_report(y_test, y_pred3))\nprint(confusion_matrix(y_test, y_pred3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ELI5","metadata":{}},{"cell_type":"code","source":"eli5.explain_weights(rf_model.named_steps[\"model\"], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_instances = rf_model.named_steps['preprocessor'].fit_transform(X_test)\neli5.explain_prediction(rf_model.named_steps[\"model\"], prep_instances[42] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.explain_prediction(rf_model.named_steps[\"model\"], prep_instances[0] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHAP\n","metadata":{}},{"cell_type":"code","source":"explainer3 = shap.TreeExplainer(rf_model.named_steps[\"model\"])\nobservations3 = rf_model.named_steps[\"preprocessor\"].transform(X_train.sample(1000, random_state=42))\nshap_values3 = explainer3.shap_values(observations3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nshap.force_plot(explainer3.expected_value[i], shap_values3[i], \n                features=observations3, feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values3, features=observations3, feature_names=all_features, max_display=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. XGBoost ","metadata":{}},{"cell_type":"code","source":"from xgboost.sklearn import XGBClassifier\n\nxgb_model = Pipeline([(\"preprocessor\", preprocessor), \n                      # Add a scale_pos_weight to make it balanced\n                     (\"model\", XGBClassifier(scale_pos_weight=(1 - y.mean()), n_jobs=-1))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs4 = GridSearchCV(xgb_model, {\"model__max_depth\": [5, 10],\n                              \"model__min_child_weight\": [10, 15],\n                              \"model__n_estimators\": [15, 25]},\n                  n_jobs=-1, \n                  cv=5,\n                  scoring=\"precision\")\n\ngs4.fit(X_train, y_train)\nprint(gs4.best_params_)\nprint(gs4.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_model.set_params(**gs4.best_params_)\nxgb_model.fit(X_train, y_train)\ny_pred4 = xgb_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame({'Method':['XGBoost'], \n                        'accuracy': accuracy_score(y_test, y_pred4), \n                        'precision': precision_score(y_test, y_pred4, pos_label=1),\n                        'recall': recall_score(y_test, y_pred4, pos_label=1),\n                        'f1': f1_score(y_test, y_pred4, pos_label=1)\n                      })\nresults = pd.concat([results, temp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y_test, y_pred4))\nprint(classification_report(y_test, y_pred4))\nprint(confusion_matrix(y_test, y_pred4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ELI5","metadata":{}},{"cell_type":"code","source":"eli5.show_weights(xgb_model.named_steps[\"model\"], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_instances4 = xgb_model.named_steps['preprocessor'].fit_transform(X_test)\neli5.explain_prediction(xgb_model.named_steps[\"model\"], prep_instances4[42] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.explain_prediction(xgb_model.named_steps[\"model\"], prep_instances4[0] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHAP","metadata":{}},{"cell_type":"markdown","source":"Since passing whole dataset to shap would cause too much computational power, I sampled 1000 random samples.","metadata":{}},{"cell_type":"code","source":"explainer4 = shap.TreeExplainer(xgb_model.named_steps[\"model\"])\nobservations4 = xgb_model.named_steps[\"preprocessor\"].transform(X_train.sample(1000, random_state=42))\nshap_values4 = explainer4.shap_values(observations4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nshap.force_plot(explainer4.expected_value, shap_values4[i], \n                features=observations4[i], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer4.expected_value, shap_values4,\n                features=observations4, feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values4, features=observations4, feature_names=all_features, max_display=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nsvc_model = Pipeline([(\"preprocessor\", preprocessor), \n                      (\"model\", LinearSVC(max_iter=5000, random_state=42, dual = False))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_model.fit(X_train, y_train)\ny_pred5 = svc_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame({'Method':['LinearSVC'], \n                        'accuracy': accuracy_score(y_test, y_pred5), \n                        'precision': precision_score(y_test, y_pred5, pos_label=1),\n                        'recall': recall_score(y_test, y_pred5, pos_label=1),\n                        'f1': f1_score(y_test, y_pred5, pos_label=1)\n                      })\nresults = pd.concat([results, temp])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(y_test, y_pred5))\nprint(classification_report(y_test, y_pred5))\nprint(confusion_matrix(y_test, y_pred5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.show_weights(svc_model.named_steps[\"model\"], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show test instances that are predicted to be 1\n# for i in range(0, len(y_pred6)):\n#     if y_pred6[i] == 1:\n#         print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_instances = svc_model.named_steps['preprocessor'].fit_transform(X_test)\neli5.explain_prediction(svc_model.named_steps[\"model\"], prep_instances[42] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eli5.explain_prediction(svc_model.named_steps[\"model\"], prep_instances[0] ,feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SHAP","metadata":{}},{"cell_type":"code","source":"prep3 = svc_model.named_steps['preprocessor'].fit_transform(X_train)\nexplainer6 = shap.LinearExplainer(svc_model.named_steps[\"model\"], prep3)\nobservations6 = svc_model.named_steps[\"preprocessor\"].transform(X_test.sample(1000, random_state=42))\nshap_values6 = explainer6.shap_values(observations6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 641\nshap.force_plot(explainer6.expected_value, shap_values6[i],\n                features=observations6[i], feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.force_plot(explainer6.expected_value, shap_values6,\n                features=observations6, feature_names=all_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values6, features=observations6, feature_names=all_features, max_display=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import community as community_louvain\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n# load the karate club graph\nG = nx.karate_club_graph()\n\n#first compute the best partition\npartition = community_louvain.best_partition(G)\n\n\nprint(partition)\npartition[34] = 3\nprint(partition)\n\n# draw the graph\n# pos = nx.spring_layout(G)\n# # color the nodes according to their partition\n# cmap = cm.get_cmap('viridis', max(partition.values()) + 1)\n# nx.draw_networkx_nodes(G, pos, partition.keys(), node_size=40,\n#                        cmap=cmap, node_color=list(partition.values()))\n# nx.draw_networkx_edges(G, pos, alpha=0.5)\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.sentiment.util import demo_liu_hu_lexicon\nline = \"Hello, you are amazing\"\ncount = [0, 0, 0]\nfor word in line:\n    d = {\"Positive\":0, \"Negative\":1, \"Neutral\": 2}\n    res = demo_liu_hu_lexicon(word)\n    print(res)\n    i = d[res]\n    count[i] += 1\n    \nprint(count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}